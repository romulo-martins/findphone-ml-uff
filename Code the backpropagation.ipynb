{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code The Backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta implementação foi baseada no exemplo do notebook backpropagation passo-a-passo, tanto os dados quanto as ideias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.total_weight_errors = []\n",
    "        \n",
    "    def calculate_output(self, inputs):\n",
    "        self.output = self.activation(self.net(inputs))\n",
    "        return self.output\n",
    "        \n",
    "    # Realiza o calculo de soma(W*x + b)    \n",
    "    def net(self, inputs):\n",
    "        return np.dot(self.weights, inputs) + self.bias    \n",
    "    \n",
    "    # Função de ativação, no caso estamos utilizando a função logistica f(x) = 1/(1 + exp(-x)) que gera curva sigmoide\n",
    "    def activation(self, net):\n",
    "        return 1.0 / (1.0 + np.exp(-net))\n",
    "\n",
    "    # Derivada da função logistica, se f'(x) = f(x)(1 - f(x))\n",
    "    def derivative_activation(self):\n",
    "        return self.output*(1 - self.output)\n",
    "    \n",
    "    def calculate_delta(self, target):\n",
    "        self.delta = (self.output - target)*self.derivative_activation() \n",
    "        return self.delta\n",
    "    \n",
    "    def update_weights(self, learning_rate):\n",
    "        for i in range(0, len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * self.total_weight_errors[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    LEARNING_RATE = 0.5\n",
    "    \n",
    "    # Número de entradas de dados, número de nós na camada oculta e número de nós de saida.\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "        # OBS: Dados de teste do artigo que serviu de base\n",
    "        self.hidden_layer = [ Neuron([.15, .20], .35), Neuron([.25, .30], .35) ]\n",
    "        self.output_layer = [ Neuron([.40, .45], .60), Neuron([.50, .55], .60) ]\n",
    "        \n",
    "        # As camadas são um conjunto de neuronios\n",
    "        # self.hidden_layer = self.init_layer_weights(num_inputs, num_hidden)\n",
    "        # self.output_layer = self.init_layer_weights(num_hidden, num_outputs)\n",
    "        \n",
    "    # Inicia as camadas de neuronios com pesos gerados aleatoriamente para cada neuronio     \n",
    "    def init_layer_weights(self, num_weights, num_neurons):\n",
    "        neurons_layer = []\n",
    "        for i in range(0, num_neurons):\n",
    "            weights = np.random.randn(num_weights) * 0.05 \n",
    "            bias = (np.random.randn(1) * 0.05)[0] \n",
    "            neurons_layer.append(Neuron(weights, bias))\n",
    "        return neurons_layer\n",
    "    \n",
    "    \n",
    "    def backpropagation(self, inputs_list, targets):\n",
    "        prediction = self.forward_pass(inputs)\n",
    "        print(\"Prediction: \", prediction)\n",
    "            \n",
    "        error = self.calculate_total_error(targets, prediction)\n",
    "        print(\"\\nError: \", error)\n",
    "        \n",
    "        print(\"\\n----- Pesos antes -----\")\n",
    "        self.show_weights()\n",
    "        \n",
    "        self.backward_pass(targets)\n",
    "        \n",
    "        print(\"\\n----- Pesos depois ----- \")\n",
    "        self.show_weights()\n",
    "        \n",
    "    def forward_pass(self, inputs):\n",
    "        outputs_hidden = self.feed_forward(inputs, self.hidden_layer)\n",
    "        return self.feed_forward(outputs_hidden, self.output_layer)\n",
    "    \n",
    "    def feed_forward(self, inputs, neurons):\n",
    "        outputs = []\n",
    "        for neuron in neurons:\n",
    "            outputs.append(neuron.calculate_output(inputs))\n",
    "        return outputs\n",
    "    \n",
    "    def backward_pass(self, targets):\n",
    "        self.compute_output_deltas(targets)\n",
    "        self.compute_hidden_deltas()\n",
    "        \n",
    "        # Atualização dos pesos, só pode ser executado se o calculo dos deltas for realizado antes\n",
    "        self.update_weights(self.output_layer)\n",
    "        self.update_weights(self.hidden_layer)\n",
    "        \n",
    "    # calculando deltas para a camada de saida (output layer)    \n",
    "    def compute_output_deltas(self, targets):\n",
    "        for neuron, target in zip(self.output_layer, targets):\n",
    "            neuron.calculate_delta(target)\n",
    "        \n",
    "        for o_neuron in self.output_layer:\n",
    "            for h_neuron, o_weights in zip(self.hidden_layer, o_neuron.weights):\n",
    "                total_weight_error = o_neuron.delta * h_neuron.output\n",
    "                o_neuron.total_weight_errors.append(total_weight_error)\n",
    "        \n",
    "    def update_weights(self, layer):\n",
    "        for neuron in layer:\n",
    "            neuron.update_weights(self.LEARNING_RATE)\n",
    "        \n",
    "    def compute_total_deltas_weights(self, index):\n",
    "        total_delta_weight = 0\n",
    "        for neuron in nn.output_layer:\n",
    "            total_delta_weight += neuron.delta * neuron.weights[index]\n",
    "        return total_delta_weight\n",
    "    \n",
    "    def compute_hidden_deltas(self):\n",
    "        for i, neuron in enumerate(self.hidden_layer):\n",
    "            for j in range(0, len(neuron.weights)):\n",
    "                total_deltas_weights = self.compute_total_deltas_weights(i)            \n",
    "                total_weight_error = total_deltas_weights * neuron.derivative_activation() * inputs[j]\n",
    "                neuron.total_weight_errors.append(total_weight_error)\n",
    "    \n",
    "    def calculate_error(self, target, output):\n",
    "        return (0.5)*(target - output)**2\n",
    "    \n",
    "    def calculate_total_error(self, targets, outputs):\n",
    "        total_error = 0.0\n",
    "        for target, output in zip(targets, outputs):\n",
    "            total_error += self.calculate_error(target, output)\n",
    "        return total_error\n",
    "    \n",
    "    def show_weights(self):\n",
    "        print(\"\\nOutput Layer: \")\n",
    "        for neuron in self.output_layer:\n",
    "            print(neuron.weights)\n",
    "        \n",
    "        print(\"\\nHidden Layer: \")\n",
    "        for neuron in self.hidden_layer:\n",
    "            print(neuron.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar, utilizaremos os seguintes dados do exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [0.7513650695523157, 0.7729284653214625]\n",
      "\n",
      "Error:  0.2983711087600027\n",
      "\n",
      "----- Pesos antes -----\n",
      "\n",
      "Output Layer: \n",
      "[0.4, 0.45]\n",
      "[0.5, 0.55]\n",
      "\n",
      "Hidden Layer: \n",
      "[0.15, 0.2]\n",
      "[0.25, 0.3]\n",
      "\n",
      "----- Pesos depois ----- \n",
      "\n",
      "Output Layer: \n",
      "[0.35891647971788465, 0.4086661860762334]\n",
      "[0.5113012702387375, 0.5613701211079891]\n",
      "\n",
      "Hidden Layer: \n",
      "[0.1497807161327628, 0.19956143226552567]\n",
      "[0.24975114363236958, 0.29950228726473915]\n"
     ]
    }
   ],
   "source": [
    "inputs = [.05, .10]\n",
    "targets = [.01, .99]\n",
    "\n",
    "nn = NeuralNetwork(num_inputs=2, num_hidden=2, num_outputs=2)\n",
    "nn.backpropagation(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atualização de pesos para Output Layer:\n",
    "[0.358916479, 0.408666186]\n",
    "[0.511301270, 0.561370121]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atualização de pesos para Hidden Layer:\n",
    "[0.149780716, 0.19956143]\n",
    "[0,24975114, 0,29950229]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
