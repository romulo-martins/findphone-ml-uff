{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code The Backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta implementação foi baseada no exemplo do notebook backpropagation passo-a-passo, tanto os dados quanto as ideias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def calculate_output(self, inputs):\n",
    "        self.output = self.activation(self.net(inputs))\n",
    "        return self.output\n",
    "        \n",
    "    # Realiza o calculo de soma(W*x + b)    \n",
    "    def net(self, inputs):\n",
    "        return np.dot(self.weights, inputs) + self.bias    \n",
    "    \n",
    "    # Função de ativação, no caso estamos utilizando a função logistica f(x) = 1/(1 + exp(-x)) que gera curva sigmoide\n",
    "    def activation(self, net):\n",
    "        return 1.0 / (1.0 + np.exp(-net))\n",
    "\n",
    "    # Derivada da função logistica, se f'(x) = f(x)(1 - f(x))\n",
    "    def derivative_activation(self, out):\n",
    "        return out*(1 - out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    LEARNING_RATE = 0.5\n",
    "    \n",
    "    # Número de entradas de dados, número de nós na camada oculta e número de nós de saida.\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "        # OBS: Dados de teste do artigo que serviu de base\n",
    "        self.hidden_layer = [ Neuron([.15, .20], .35), Neuron([.25, .30], .35) ]\n",
    "        self.output_layer = [ Neuron([.40, .45], .60), Neuron([.50, .55], .60) ]\n",
    "        \n",
    "        # self.hidden_layer = self.init_layer(num_inputs, num_hidden)\n",
    "        # self.output_layer = self.init_layer(num_hidden, num_outputs)\n",
    "        \n",
    "    # Inicia as camadas de neuronios com pesos gerados aleatoriamente para cada neuronio     \n",
    "    def init_layer_weights(self, num_weights, num_neurons):\n",
    "        neurons_layer = []\n",
    "        for i in range(0, num_neurons):\n",
    "            weights = np.random.randn(num_weights) * 0.05 \n",
    "            bias = (np.random.randn(1) * 0.05)[0] \n",
    "            neurons_layer.append(Neuron(weights, bias))\n",
    "        return neurons_layer\n",
    "        \n",
    "    def pass_forward(self, inputs):\n",
    "        outputs_hidden = self.feed_forward(inputs, self.hidden_layer)\n",
    "        return self.feed_forward(outputs_hidden, self.output_layer)\n",
    "        \n",
    "    def feed_forward(self, inputs, neurons):\n",
    "        outputs = []\n",
    "        for neuron in neurons:\n",
    "            outputs.append(neuron.calculate_output(inputs))\n",
    "        return outputs\n",
    "    \n",
    "    def train(self, inputs, targets):\n",
    "        \"\"\"\n",
    "            TODO:\n",
    "            pseudo\n",
    "            initialize network weights (often small random values)\n",
    "              do\n",
    "                 foreach training example, x\n",
    "                    prediction = neural-net-output(network, x)  // forward pass\n",
    "                    actual = teacher-output(x)\n",
    "                    compute error (prediction - actual) at the output units\n",
    "                    compute deltas for all weights from hidden layer to output layer  // backward pass\n",
    "                    compute deltas for all weights from input layer to hidden layer   // backward pass continued\n",
    "                    update network weights   // input layer not modified by error estimate\n",
    "              until all examples classified correctly or another stopping criterion satisfied\n",
    "              return the network\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, input):\n",
    "        \"\"\"\n",
    "            TODO: \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def calculate_error(self, target, output):\n",
    "        return (0.5)*(target - output)**2\n",
    "    \n",
    "    def calculate_total_error(self, targets, outputs):\n",
    "        total_error = 0.0\n",
    "        for target, output in zip(targets, outputs):\n",
    "            total_error += self.calculate_error(target, output)\n",
    "        return total_error\n",
    "    \n",
    "    def print_output_weights(self):\n",
    "        print(self.output_layer[0].we)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar, utilizaremos os seguintes dados do exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7513650695523157, 0.7729284653214625]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [.05, .10]\n",
    "outputs = [.01, .99]\n",
    "\n",
    "nn = NeuralNetwork(num_inputs=2, num_hidden=2, num_outputs=2)\n",
    "nn_out = nn.pass_forward(inputs)\n",
    "nn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2983711087600027"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.calculate_total_error(nn_out, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apos o backward pass os pesos devem ser alterados para os seguintes valores:\n",
    "[0.35891648, 0.408666186], [ 0.511301270, 0.561370121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4, 0.45]\n",
      "[0.5, 0.55]\n"
     ]
    }
   ],
   "source": [
    "for n in nn.output_layer:\n",
    "    print(n.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5103727736102812, 0.48429897690471635, 0.49660195034821436]\n",
      "0.25305321866503117\n"
     ]
    }
   ],
   "source": [
    "# import a dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .5)\n",
    "\n",
    "nn = NeuralNetwork(num_inputs=4, num_hidden=2, num_outputs=3)\n",
    "nn_out = nn.pass_forward(X_train[0])\n",
    "print(nn_out)\n",
    "print(nn.calculate_total_error(nn_out, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
