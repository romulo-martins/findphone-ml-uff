{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code The Backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta implementação foi baseada no exemplo do notebook backpropagation passo-a-passo, tanto os dados quanto as ideias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def calculate_output(self, inputs):\n",
    "        self.output = self.activation(self.net(inputs))\n",
    "        return self.output\n",
    "        \n",
    "    # Realiza o calculo de soma(W*x + b)    \n",
    "    def net(self, inputs):\n",
    "        return np.dot(self.weights, inputs) + self.bias    \n",
    "    \n",
    "    # Função de ativação, no caso estamos utilizando a função logistica f(x) = 1/(1 + exp(-x)) que gera curva sigmoide\n",
    "    def activation(self, net):\n",
    "        return 1.0 / (1.0 + np.exp(-net))\n",
    "\n",
    "    # Derivada da função logistica, se f'(x) = f(x)(1 - f(x))\n",
    "    def derivative_activation(self):\n",
    "        return self.output*(1 - self.output)\n",
    "    \n",
    "    def calculate_delta(self, target):\n",
    "        self.delta = (self.output - target)*self.derivative_activation() \n",
    "        return self.delta\n",
    "    \n",
    "    def update_weights(self, learning_rate, errors_total_weights):\n",
    "        for i in range(0, len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * errors_total_weights[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    LEARNING_RATE = 0.5\n",
    "    \n",
    "    # Número de entradas de dados, número de nós na camada oculta e número de nós de saida.\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "        # OBS: Dados de teste do artigo que serviu de base\n",
    "        self.hidden_layer = [ Neuron([.15, .20], .35), Neuron([.25, .30], .35) ]\n",
    "        self.output_layer = [ Neuron([.40, .45], .60), Neuron([.50, .55], .60) ]\n",
    "        \n",
    "        # self.hidden_layer = self.init_layer_weights(num_inputs, num_hidden)\n",
    "        # self.output_layer = self.init_layer_weights(num_hidden, num_outputs)\n",
    "        \n",
    "    # Inicia as camadas de neuronios com pesos gerados aleatoriamente para cada neuronio     \n",
    "    def init_layer_weights(self, num_weights, num_neurons):\n",
    "        neurons_layer = []\n",
    "        for i in range(0, num_neurons):\n",
    "            weights = np.random.randn(num_weights) * 0.05 \n",
    "            bias = (np.random.randn(1) * 0.05)[0] \n",
    "            neurons_layer.append(Neuron(weights, bias))\n",
    "        return neurons_layer\n",
    "        \n",
    "    def forward_pass(self, inputs):\n",
    "        outputs_hidden = self.feed_forward(inputs, self.hidden_layer)\n",
    "        return self.feed_forward(outputs_hidden, self.output_layer)\n",
    "    \n",
    "    def backward_pass(self, targets):\n",
    "        \"\"\"\n",
    "            TODO: \n",
    "        \"\"\"\n",
    "\n",
    "    # calculando deltas para a camada de saida (output layer)    \n",
    "    def compute_output_deltas(self, targets):\n",
    "        for neuron, target in zip(self.output_layer, targets):\n",
    "            neuron.calculate_delta(target)\n",
    "        \n",
    "    # Atualização dos pesos, só pode ser executado se o calculo dos deltas for realizado antes    \n",
    "    def update_output_weights(self):\n",
    "        for o_neuron in self.output_layer:\n",
    "            for h_neuron, o_weights in zip(self.hidden_layer, o_neuron.weights):\n",
    "                errors_total_weights = o_neuron.delta * h_neuron.output\n",
    "                \n",
    "                # update weights\n",
    "                weight_updated = o_weights - 0.5 * errors_total_weights\n",
    "                print(weight_updated)\n",
    "        \n",
    "    def compute_total_deltas_weights(self, index):\n",
    "        total_delta_weight = 0\n",
    "        for neuron in nn.output_layer:\n",
    "            total_delta_weight += neuron.delta * neuron.weights[index]\n",
    "        return total_delta_weight\n",
    "    \n",
    "    def update_hidden_weights(self):\n",
    "        for i, neuron in enumerate(self.hidden_layer):\n",
    "            for j in range(0, len(neuron.weights)):\n",
    "                total_deltas_weights = self.compute_total_deltas_weights(i)            \n",
    "                error_total_weight = total_deltas_weights * neuron.derivative_activation() * inputs[j]\n",
    "\n",
    "                # update weights\n",
    "                weight_updated = neuron.weights[j] - 0.5* error_total_weight\n",
    "                print(weight_updated)\n",
    "        \n",
    "    def feed_forward(self, inputs, neurons):\n",
    "        outputs = []\n",
    "        for neuron in neurons:\n",
    "            outputs.append(neuron.calculate_output(inputs))\n",
    "        return outputs\n",
    "    \n",
    "    def calculate_error(self, target, output):\n",
    "        return (0.5)*(target - output)**2\n",
    "    \n",
    "    def calculate_total_error(self, targets, outputs):\n",
    "        total_error = 0.0\n",
    "        for target, output in zip(targets, outputs):\n",
    "            total_error += self.calculate_error(target, output)\n",
    "        return total_error\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para testar, utilizaremos os seguintes dados do exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2983711087600027"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [.05, .10]\n",
    "targets = [.01, .99]\n",
    "\n",
    "nn = NeuralNetwork(num_inputs=2, num_hidden=2, num_outputs=2)\n",
    "nn_out = nn.forward_pass(inputs)\n",
    "nn.calculate_total_error(nn_out, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Hidden Layer outputs -----\n",
      "0.5932699921071872\n",
      "0.596884378259767\n"
     ]
    }
   ],
   "source": [
    "print(\"----- Hidden Layer outputs -----\")\n",
    "for n in nn.hidden_layer:\n",
    "    print(n.output)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Output Layer outputs -----\n",
      "0.7513650695523157\n",
      "0.7729284653214625\n"
     ]
    }
   ],
   "source": [
    "print(\"----- Output Layer outputs -----\")\n",
    "for n in nn.output_layer:\n",
    "    print(n.output)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apos o backward pass os pesos da Output Layer devem ser alterados para os seguintes valores:\n",
    "[0.35891648, 0.408666186], [ 0.511301270, 0.561370121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Output Layer Weights -----\n",
      "[0.4, 0.45]\n",
      "[0.5, 0.55]\n"
     ]
    }
   ],
   "source": [
    "print(\"----- Output Layer Weights -----\")    \n",
    "for n in nn.output_layer:\n",
    "    print(n.weights)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apos o backward pass os pesos da Hidden Layer devem ser alterados para os seguintes valores:\n",
    "[,], [,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Hidden Layer Weights -----\n",
      "[0.15, 0.2]\n",
      "[0.25, 0.3]\n"
     ]
    }
   ],
   "source": [
    "print(\"----- Hidden Layer Weights -----\")    \n",
    "for n in nn.hidden_layer:\n",
    "    print(n.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35891647971788465\n",
      "0.4086661860762334\n",
      "0.5113012702387375\n",
      "0.5613701211079891\n"
     ]
    }
   ],
   "source": [
    "nn.compute_output_deltas(targets)\n",
    "nn.update_output_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1497807161327628\n",
      "0.19956143226552567\n",
      "0.24975114363236958\n",
      "0.29950228726473915\n"
     ]
    }
   ],
   "source": [
    "nn.update_hidden_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
